import scanpy as sc
import scvi

adata = sc.read_h5ad("/data1/BLA_projection_2/Data/Resources/.v2/after_preprocessing/adata_all_concat.h5ad")

adata.layers["counts"] = adata.X.copy()


sc.pp.normalize_total(adata, target_sum=1e4)
sc.pp.log1p(adata)

adata.layers["logcounts"] = adata.X.copy()

sc.pp.highly_variable_genes(adata)
sc.tl.pca(adata)
sc.pp.neighbors(adata)
sc.tl.umap(adata)
adata

sc.pl.umap(adata, color=["sampleID"], wspace=1)
batch_key = "sampleID"
sc.pl.umap(adata, color=[batch_key], wspace=1)

# ë¯¸í† ì½˜ë“œë¦¬ì•„ ìœ ì „ì ë¹„ìœ¨ ê³„ì‚°
adata.var['mt'] = adata.var_names.str.startswith('mt-')
sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True)

# QC metric ì‹œê°í™”
sc.pl.violin(adata, ['total_counts', 'n_genes_by_counts', 'pct_counts_mt'], groupby='sample', multi_panel=True)
sc.pl.umap(adata, color=['sample', 'total_counts', 'pct_counts_mt'])




# highly variable genes ì„ íƒ
sc.pp.highly_variable_genes(adata, flavor="seurat", n_top_genes=2000, subset=True, batch_key="sampleID")

# scale
sc.pp.scale(adata, max_value=10)



n_batches = adata.var["highly_variable_nbatches"].value_counts()
ax = n_batches.plot(kind="bar")


# 1. í˜„ì¬ ìƒíƒœì˜ adata (HVG 2000ê°œë§Œ í¬í•¨)ì—ì„œ ë³µì‚¬
adata_scvi = adata.copy()

scvi.model.SCVI.setup_anndata(adata_scvi, layer="counts", batch_key="sampleID")
adata_scvi

import torch
print(torch.cuda.is_available())  # Falseì´ë©´ CPUë¡œë§Œ ì§„í–‰ë¨


model_scvi = scvi.model.SCVI(adata_scvi)
model_scvi

model_scvi.view_anndata_setup()

import numpy as np

max_epochs_scvi = np.min([round((20000 / adata.n_obs) * 400), 400])
max_epochs_scvi


model_scvi.train()

adata_scvi.obsm["X_scVI"] = model_scvi.get_latent_representation()

sc.pp.neighbors(adata_scvi, use_rep="X_scVI")
sc.tl.umap(adata_scvi)
adata_scvi

sc.pl.umap(adata_scvi,  color=['sample', 'pct_counts_mt', 'total_counts'], title='scVI UMAP')

from sklearn.metrics import silhouette_score

resolutions = [0.2, 0.4, 0.6, 0.8, 1.0]
scores = {}

for res in resolutions:
    sc.tl.leiden(adata_scvi, resolution=res, key_added=f'leiden_{res}')
    score = silhouette_score(adata_scvi.obsm['X_scVI'], adata_scvi.obs[f'leiden_{res}'])
    scores[res] = score
    print(f"ğŸ”¹ resolution={res} â†’ silhouette_score={score:.4f}")

# ìµœì  resolution ì ìš©
best_res = max(scores, key=scores.get)
sc.tl.leiden(adata_scvi, resolution=best_res, key_added='leiden')
sc.pl.umap(adata_scvi, color='leiden', title=f"Leiden (res={best_res})")


# ğŸ”¹ í´ëŸ¬ìŠ¤í„°ë³„ marker gene íƒìƒ‰
sc.tl.rank_genes_groups(
    adata_scvi,
    groupby='leiden_0.2',
    method='wilcoxon',  # ë˜ëŠ” 't-test', 'logreg'
    key_added='rank_genes_leiden_0.6'
)

# ğŸ”¹ ìƒìœ„ marker ìœ ì „ì ì‹œê°í™”
sc.pl.rank_genes_groups(
    adata_scvi,
    key='rank_genes_leiden_0.6',
    n_genes=5,  # í´ëŸ¬ìŠ¤í„°ë‹¹ ìƒìœ„ 5ê°œ
    sharey=False
)


# Celltypistìš© AnnData ë³µì‚¬ë³¸ ìƒì„± (ì›ë³¸ ìœ ì§€)
adata_celltypist = adata_scvi.copy()

# raw count layerë¥¼ Xë¡œ ì„¤ì •
adata_celltypist.X = adata_celltypist.layers["counts"].copy()

# ì„¸í¬ ë‹¨ìœ„ ì •ê·œí™” (1ë§Œ countsë¡œ ë§ì¶¤)
import scanpy as sc
sc.pp.normalize_total(adata_celltypist, target_sum=1e4)

# log1p ë³€í™˜
sc.pp.log1p(adata_celltypist)

# í¬ì†Œí–‰ë ¬ì„ dense matrixë¡œ ë³€í™˜ (celltypistëŠ” dense ì…ë ¥ ìš”êµ¬)
import numpy as np
if not isinstance(adata_celltypist.X, np.ndarray):
    adata_celltypist.X = adata_celltypist.X.toarray()


from celltypist import models


models.download_models(
    force_update=True, 
    model=["Mouse_Whole_Brain.pkl"]
)

model_1 = models.Model.load(model="Mouse_Whole_Brain.pkl")

model_1.cell_types

import celltypist


predictions_high = celltypist.annotate(
    adata_celltypist, 
    model=model_1, 
    majority_voting=True
)

# ì˜ˆì¸¡ ë¼ë²¨ì˜ ì¸ë±ìŠ¤ ì¤‘ë³µ ì—¬ë¶€ í™•ì¸
pred_idx = predictions_high.predicted_labels.index
duplicated = pred_idx.duplicated()
print(f"ì¤‘ë³µ ì¸ë±ìŠ¤ ìˆ˜: {duplicated.sum()}")
print(pred_idx[duplicated])


# ì „ì²´ ì…€ ìˆ˜
total_cells = len(predictions_high.predicted_labels)

# ì¤‘ë³µ ì¸ë±ìŠ¤ ê°œìˆ˜
duplicated_cells = predictions_high.predicted_labels.index.duplicated(keep=False).sum()

# ì¤‘ë³µ ë¹„ìœ¨
dup_ratio = duplicated_cells / total_cells * 100

print(f"ì „ì²´ ì…€ ìˆ˜: {total_cells}")
print(f"ì¤‘ë³µëœ ì¸ë±ìŠ¤ ìˆ˜: {duplicated_cells}")
print(f"ì¤‘ë³µ ë¹„ìœ¨: {dup_ratio:.2f}%")


# ì˜ˆì¸¡ ë¼ë²¨ì˜ ì¸ë±ìŠ¤ ì¤‘ë³µ ì—¬ë¶€ í™•ì¸
pred_idx = predictions_high.predicted_labels.index
duplicated = pred_idx.duplicated()
print(f"ì¤‘ë³µ ì¸ë±ìŠ¤ ìˆ˜: {duplicated.sum()}")
print(pred_idx[duplicated])


# ì¤‘ë³µëœ indexë§Œ ì¶”ì¶œ
duplicated_index = adata.obs_names[adata.obs_names.duplicated(keep=False)]

# ì¤‘ë³µ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” obs ë°ì´í„° í™•ì¸
duplicated_obs = adata.obs.loc[duplicated_index]

# ì¤‘ë³µ ì¸ë±ìŠ¤ë“¤ì˜ sampleIDë¥¼ ì§‘ê³„
duplicated_obs['sampleID'].value_counts()


# ì¤‘ë³µëœ ì¸ë±ìŠ¤ì™€ sampleID ë§¤ì¹­ í™•ì¸
duplicated_obs[['sampleID']].groupby(duplicated_obs.index).value_counts()

# ì¤‘ë³µëœ index ì¤‘ ì²« ë²ˆì§¸ ê²ƒë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ì œê±°
predictions_high.predicted_labels = predictions_high.predicted_labels[~predictions_high.predicted_labels.index.duplicated()]

predictions_high.to_adata()

# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ AnnDataë¡œ ë³€í™˜
predictions_high_adata = predictions_high.to_adata()

# indexê°€ ì¤‘ë³µë˜ì§€ ì•Šë„ë¡ ì •ë¦¬ (ì˜ˆë°© ì°¨ì›)
predictions_high_adata = predictions_high_adata[~predictions_high_adata.obs.index.duplicated(), :]

# celltypist ê²°ê³¼ë¥¼ adata.obsì— ì¶”ê°€
adata_scvi.obs["celltypist_cell_label_coarse"] = predictions_high_adata.obs.loc[adata.obs.index, "majority_voting"]
adata_scvi.obs["celltypist_conf_score_coarse"] = predictions_high_adata.obs.loc[adata.obs.index, "conf_score"]

adata_scvi.obs["celltypist_cell_label_coarse"] = predictions_high_adata.obs["majority_voting"]
adata_scvi.obs["celltypist_conf_score_coarse"] = predictions_high_adata.obs["conf_score"]



sc.pl.umap(
    adata_scvi,
    color=["celltypist_cell_label_coarse", "celltypist_conf_score_coarse"],
    frameon=False,
    sort_order=False,
    wspace=1,
)